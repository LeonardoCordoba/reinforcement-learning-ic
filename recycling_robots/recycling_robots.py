# -*- coding: utf-8 -*-
"""TP1-version final con nuevo gamma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FPdnX1GIMzcqhw4u5r8Fxvq203z88Rdd

**Recycling robot example** (from Sutton, page 42)
References:
  - Gym documentation: https://gym.openai.com/
"""

import numpy as np
from gym.envs.toy_text import discrete
import random
import matplotlib.pyplot as plt

"""## Describir coloquialmente el modelo de sutton
### Modelo - Robot de reciclaje

Este modelo consta de un **agente** (robot) que tiene por fin buscar latas. El agente tiene una cantidad de energía finita, y así definimos dos posibles estados: de batería baja (**low**) o de batería alta (**high**).

En cada estado el robot puede elegir entre buscar latas (lo cual reduce su batería) o esperar si alguien le trae una (en este caso se mantiene el nivel de batería). Además, en el estado de batería baja puede recargar su batería.

Esperar siempre tiene asociado un reward **r_wait**, sin importar en el estado en que se está.

Buscar latas en el estado **high** tiene asociado un reward **r_search** y mantiene al robot con baterías altas (es decir, en estado **high**) con probabilidad **alpha**  y pasa a tener baterías bajas con probabilidad **1 - alpha**.  En cambio, si el robot está en **low**, entonces al buscar puede mantenerse en low con probabilidad **beta** y se queda sin batería con probabilidad **1 - beta**. En este último caso el robot tiene que ser recuperado, con lo el reward es negativo en este caso: -3.

Por su parte, el reward de recargar las baterías es de 0.

Como se dijo antes, alpha y beta controlan la probabilidad de, dado que la acción es buscar, se mantenga en el mismo estado. Con lo cual, es de esperar que cuanto más alto sean alpha y beta mayor sea la propensión del robot a buscar, ya que la probabilidad de quedarse sin baterías va a ser menor.

## Explicar lo básico de GYM

GYM es una biblioteca desarrollada por Open.ai que facilita el entrenamiento de algoritmos de aprendizaje por refuerzo en diferentes entornos.

GYM incluye una serie de ambientes que permiten realizar pruebas en distintos entornos, como juegos de Atari, simulación de robots 2d y 3d, aprender tareas algorítmicas como sumar números o invertir secuencias a partir de ejemplos.

 Los ambientes cuentan con los atributos action_space y observation_space que describen las posibles acciones y estados, respectivamente. Además, cuentan con una función step, que permite tomar una acción y ver en qué nuevo estado se está y el reward obtenido.

# Considere el modelo del robot de reciclaje descríto en Sutton Example 3.2
"""

states = ["high", "low"]
actions = ["wait", "search", "recharge"]

alpha = 0.8 #1 
beta = 0.1 #1
r_wait = 0.5
r_search = 2

# definimos un ambiente discreto con las transiciones según el gráfico
def generar_ambiente(alpha=alpha, beta=beta, r_wait=r_wait, r_search=r_search):
  P = {}
  P[0] = {}
  P[1] = {}

  P[0][0] = [(1.0, 0, r_wait, False)]  
  P[0][1] = [(alpha, 0, r_search, False),
             (1-alpha, 1, r_search, False)]
  P[0][2] = [(1,0,0,False)]

  P[1][0] = [(1.0, 1, r_wait, False)]
  P[1][1] = [(beta, 1, r_search, False), 
             (1-beta, 0, -3.0, False)]
  P[1][2] = [(1.0, 0, 0.0, False)]
  env_res = discrete.DiscreteEnv(2, 3, P, [0.0, 1.0])
  return(env_res)
env = generar_ambiente()

"""# Implemente la estrategia random para veinte episodios.

Definir una acción aleatoria y ver que reward produce
"""

print("comienzo en el estado:", env.reset())
reward_history = []
reward_acum = 0
gamma = 0.01

# 20 iteraciones
for i in range(20):
  
  # Elijo una acción al azar
  action = env.action_space.sample()
  
  # Si estoy en el estado high y tomo recharge, vuelvo a elegir
  while (action == 2 and observation == 0):
    action = env.action_space.sample()
  
  print("tomo la accion:", action)
  
  # Tomo la acción elegida
  observation, reward, done, info = env.step(action)
  
  # Actualizo el reward acumulado
  reward_acum = reward_acum + (gamma**i) * reward
  
  print("tengo un reward:", reward, " y obtengo estado:", observation, " y reward acum", reward_acum)
  
  # Guardo el reward en una lista
  reward_history.append((i, reward, reward_acum))
env.close()

"""# Grafique la recompensa acumulada"""

import plotly.plotly as py
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot

init_notebook_mode(connected=True)

# Esta función es necesaria para ejecutar plotly en Collaboratory, en Jupyter notebook no
def enable_plotly_in_cell():
  import IPython
  from plotly.offline import init_notebook_mode
  display(IPython.core.display.HTML('''<script src="/static/components/requirejs/require.js"></script>'''))
  init_notebook_mode(connected=False)

it = [i[0] for i in reward_history]
re = [i[2] for i in reward_history]

trace0 = go.Scatter(
    x = it,
    y = re,
    mode = 'lines',
    name = 'reward'
)

layout = go.Layout(
    title=go.layout.Title(
        text='Reward acumulado',
        xref='paper',
        x=0
    ),
    xaxis=go.layout.XAxis(
        title=go.layout.xaxis.Title(
            text='Iteración',
            font=dict(
                family='Courier New, monospace',
                size=18,
                color='#7f7f7f'
            )
        )
    ),
    yaxis=go.layout.YAxis(
        title=go.layout.yaxis.Title(
            text='Reward acumulado',
            font=dict(
                family='Courier New, monospace',
                size=18,
                color='#7f7f7f'
            )
        )
    )
)

data = [trace0]
fig = go.Figure(data=data, layout=layout)

enable_plotly_in_cell() # Nota: solamente es necesario si se ejecuta en Collaboratory

iplot(fig, filename='reward-acum.html')

"""# Calcule de forma teórica V, la value function optima para cada estado

Queremos calcular el valor teórico de $V$ para todas las posibles políticas determinísticas $\pi$. Para esto utilizaremos la ecuación de Bellman para $V^{\pi}$, matriz de transición $P^{\pi}$ para dicha política, vector de recompensas $R^{\pi}$ y factor de descuento $\gamma$

$V^{\pi} = R^{\pi} + \gamma P^{\pi} V^{\pi}$

$V^{\pi} - \gamma P^{\pi} V^{\pi} = R^{\pi}$

$(I - \gamma P^{\pi}) V^{\pi}  = R^{\pi}$

Si asumimos que el término $(I - \gamma P^{\pi})$ es inversible (lo es) obtenemos que

$V^{\pi}  = (I - \gamma P^{\pi})^{-1} R^{\pi}$

Queremos calcular el valor teórico de $V$ para todas las posibles políticas determinísticas $\pi$. Para esto utilizaremos la ecuación de Bellman para $V^{\pi}$, matriz de transición $P^{\pi}$ para dicha política, vector de recompensas $R^{\pi}$ y factor de descuento $\gamma$

$V^{\pi} = R^{\pi} + \gamma P^{\pi} V^{\pi}$

$V^{\pi} - \gamma P^{\pi} V^{\pi} = R^{\pi}$

$(I - \gamma P^{\pi}) V^{\pi}  = R^{\pi}$

Si asumimos que el término $(I - \gamma P^{\pi})$ es inversible (lo es) obtenemos que

$V^{\pi}  = (I - \gamma P^{\pi})^{-1} R^{\pi}$

Consideremos todas las políticas posibles, donde $\pi \in \mathbb{R}^{2\times 3}$ ya que tenemos dos estados y tres acciones posibles. Para cada una de esas políticas, por ejemplo:

$
\pi_1 =
  \begin{pmatrix}
    1 & 0 & 0  \\
    1 & 0 & 0 
  \end{pmatrix}
$

$\pi_1$ es la política que para ambos estados tomo la primer acción. El espacio de los estados es $S=\{ \text{"high", "low"} \}$ y el de las acciones $A = \{ \text{"wait", "search", "recharge"} \}$, calculamos entonces para $\alpha = \beta = 1$ y $r_{wait} = 0.5$ y $r_{search}=2$.

Para $\pi_1$:

$P^{\pi_1} =  \begin{pmatrix}
    1 & 0  \\
    0 & 1
  \end{pmatrix}$ y 
$R^{\pi_1} = \begin{pmatrix}
    0.5  \\
    0.5
  \end{pmatrix}$

Entonces:

$V^{\pi_1} = \bigg[\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} - 
0.01 \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\bigg]^{-1} \begin{pmatrix} 0.5 \\
0.5 \end{pmatrix} = \begin{pmatrix} 0.505 \\ 0.505 \end{pmatrix} $

Caso $\pi_2$:

$
\pi_2 =
  \begin{pmatrix}
    1 & 0 & 0  \\
    0 & 1 & 0 
  \end{pmatrix}
$

$P^{\pi_2} =  \begin{pmatrix}
    1 & 0  \\
    1-\beta & \beta
  \end{pmatrix} = 
	\begin{pmatrix}
    1 & 0  \\
    0 & 1
  \end{pmatrix}$ y 
$R^{\pi_2} = \begin{pmatrix}
    0.5  \\
    2
  \end{pmatrix}$

Entonces:

$V^{\pi_2} = \bigg[\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} - 
0.01 \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\bigg]^{-1} \begin{pmatrix} 0.5 \\
2 \end{pmatrix} = \begin{pmatrix} 0.505 \\ 2.02 \end{pmatrix} $

Caso $\pi_3$:

$
\pi_3 =
  \begin{pmatrix}
    1 & 0 & 0  \\
    0 & 0 & 1 
  \end{pmatrix}
$

$P^{\pi_3} =  \begin{pmatrix}
    1 & 0  \\
    1 & 0
  \end{pmatrix}$ y 
$R^{\pi_3} = \begin{pmatrix}
    0.5  \\
    0
  \end{pmatrix}$

Entonces:

$V^{\pi_3} = \bigg[\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} - 
0.01 \begin{pmatrix} 1 & 0 \\ 1 & 0 \end{pmatrix}\bigg]^{-1} \begin{pmatrix} 0.5 \\
0 \end{pmatrix} = \begin{pmatrix} 0.505 \\ 0.00505 \end{pmatrix} $

Caso $\pi_4$:

$
\pi_4 =
  \begin{pmatrix}
    0 & 1 & 0  \\
    1 & 0 & 0 
  \end{pmatrix}
$

$P^{\pi_4} =  \begin{pmatrix}
    \alpha & 1-\alpha  \\
    0 & 1
  \end{pmatrix} = 
\begin{pmatrix}
    1 & 0  \\
    0 & 1
  \end{pmatrix}$ y 
$R^{\pi_4} = \begin{pmatrix}
    2 \\
    2
  \end{pmatrix}$

Entonces:

$V^{\pi_4} = \bigg[\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} - 
0.01 \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\bigg]^{-1} \begin{pmatrix} 2 \\
2 \end{pmatrix} = \begin{pmatrix} 2.0202 \\ 2.0202 \end{pmatrix} $

Caso $\pi_5$:

$
\pi_5 =
  \begin{pmatrix}
    0 & 1 & 0  \\
    0 & 1 & 0 
  \end{pmatrix}
$

$P^{\pi_5} =  \begin{pmatrix}
    1 & 0  \\
    0 & 1
  \end{pmatrix}$ y 
$R^{\pi_5} = \begin{pmatrix}
    2  \\
    0.5
  \end{pmatrix}$

Entonces:

$V^{\pi_5} = \bigg[\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} - 
0.01 \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\bigg]^{-1} \begin{pmatrix} 2 \\
0.5 \end{pmatrix} = \begin{pmatrix} 2.0202 \\ 0.505 \end{pmatrix} $

Caso $\pi_6$:

$
\pi_6 =
  \begin{pmatrix}
    0 & 1 & 0  \\
    0 & 0 & 1 
  \end{pmatrix}
$

$P^{\pi_6} =  \begin{pmatrix}
    1 & 0  \\
    1 & 0
  \end{pmatrix}$ y 
$R^{\pi_6} = \begin{pmatrix}
    2  \\
    0
  \end{pmatrix}$

Entonces:

$V^{\pi_6} = \bigg[\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} - 
0.01 \begin{pmatrix} 1 & 0 \\ 1 & 0 \end{pmatrix}\bigg]^{-1} \begin{pmatrix} 2 \\
0 \end{pmatrix} = \begin{pmatrix} 2.02 \\ 0.0202 \end{pmatrix} $

Por lo tanto la política óptima es $\pi_4$ en la cual para ambos estados realizo la acción "search" con un valor de 2.02 para ambas acciones.

# Implemente el algoritmo de iteración de valor (Value iteration)

Evaluate the optimal value function given a full description of the environment dynamics
  
  

```
 Args:

        env: OpenAI env. env.P represents the transition probabilities of the environment.
            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).
            env.nS is a number of states in the environment. 
            env.nA is a number of actions in the environment.
        theta: We stop evaluation once our value function change is less than theta for all states.
        discount_factor: Gamma discount factor.
  
  Returns:
        Vector of length env.nS representing the value function.
```
"""

def value_iteration(env, theta, discount_factor):
  
  ## Paso 1: obtengo V* 
  
  # Inicializo V y delta
  V = np.zeros(env.nS)
  
  states = [i for i in range(env.nS)]
  actions = [i for i in range(env.nA)]
  control = True
  
  while control:
    delta = 0
    
    # Por cada estado
    for s in states:
    # Obtengo el valor del estado
      v = V[s]
      action_values = []
      
      # Por cada accion
      for a in actions:
        action_values_aux = []
        
        # Obtengo el valor del juego
        for s_ in [i for i in env.P[s][a]]:          
          action_values_aux.append((s_[0] * (s_[2] + discount_factor*V[s_[1]])))
        
        action_values.append(np.sum(action_values_aux))
                
      # Obtengo el valor maximo del juego en el estado s
      best_action_v = np.max(action_values)

      # Calculo delta     
      delta = max(delta, abs(best_action_v - V[s]))

      V[s] = best_action_v

    # Veo si se cumple la condición de corte
    if delta < theta:
      control = False
  
  
  # Pas 2: Obtengo politica deterministica

  policy = {}
  
  # Por cada estado
  for s in states:
    action_values = []

    # Por cada accion
    for a in actions:
      action_values_aux = []

      # Obtengo el valor del juego
      for s_ in [i for i in env.P[s][a]]:
        action_values_aux.append((s_[0] * (s_[2] + discount_factor*V[s_[1]])))

      action_values.append(np.sum(action_values_aux))
    
    # Me quedo con la acción que maximiza
    policy[s] = np.argmax(action_values)
  
  # Transformo el resultado de un diccionario a una lista
  final_policy = np.zeros((2,3))
  for i in policy.keys():
    final_policy[i][policy[i]] = 1
  return V, final_policy

env.P

theta = 0.0001
discount_factor = 0.01
value_iteration(env, theta, discount_factor)

"""# Implemente el algoritmo de policy iteration

Definir primero una funcion de evaluación de politica,

```
Evaluate a policy given an environment and a full description of the environment's dynamics.
    
    Args:
        policy: [S, A] shaped matrix representing the policy.
        env: OpenAI env. env.P represents the transition probabilities of the environment.
            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).
            env.nS is a number of states in the environment. 
            env.nA is a number of actions in the environment.
        theta: We stop evaluation once our value function change is less than theta for all states.
        discount_factor: Gamma discount factor.
    
    Returns:
        Vector of length env.nS representing the value function.
        
```

Despues una funcion de optimisacion de la politica:


```
 Policy Improvement Algorithm. Iteratively evaluates and improves a policy
    until an optimal policy is found.
    
    Args:
        env: The OpenAI envrionment.
        policy_eval_fn: Policy Evaluation function that takes 3 arguments:
            policy, env, discount_factor.
        discount_factor: gamma discount factor.
        
    Returns:
        A tuple (policy, V). 
        policy is the optimal policy, a matrix of shape [S, A] where each state s
        contains a valid probability distribution over actions.
        V is the value function for the optimal policy.
        
```

### Policy evaluation
"""

def policy_evaluation(policy, env, theta, discount_factor):
  states = [i for i in range(env.nS)]
  actions = [i for i in range(env.nA)]
  control = True
  V = np.zeros(env.nS)

  while control:
    delta = 0
    
      # Por cada estado
    for s in states:

    # Obtengo el valor del estado
      v = V[s]
      action_values = []

      # Por cada accion
      for a in actions:
        action_values_aux = []
        
        # Obtengo el valor del juego
        for s_ in [i for i in env.P[s][a]]:          
          action_values_aux.append((s_[0] * (s_[2] + discount_factor*V[s_[1]])))
        
        a_reward = np.sum(action_values_aux)
        
        # Probabilidad de a dado s
        a_prob_given_s = policy[s][a]
        
        # Guardo el producto
        action_values.append(a_prob_given_s * a_reward)
      
      # Sumo el valor esperado para la acción y lo guardo
      V[s] = np.sum(action_values)
      
      delta = max(delta, abs(v - V[s]))
    
    # Me fijo si se cumple la condición de corte
    if delta < theta:
      control = False
  
  return V

# Defino una política de prueba
policy = np.array([[0,0.3,0.7], 
                  [0.7,0.3,0]])

policy_evaluation(policy, env, theta, discount_factor)

"""Para ver si está bien controlo que me dé el mismo valor que arriba. Pruebo usando la política óptima encontrada antes."""

value_iteration(env, theta, discount_factor)[1]

# Defino la política óptima
policy = np.array([[0,1,0], 
                  [1,0,0]])

policy_evaluation(policy, env, theta, discount_factor)

np.array_equal(policy_evaluation(policy, env, theta, discount_factor), value_iteration(env, theta, discount_factor)[0])

"""### Policy iteration"""

def policy_iteration(env, policy_eval_fn, discount_factor):
  states = [i for i in range(env.nS)]
  actions = [i for i in range(env.nA)]
  policy = np.array([[1,0,0], [1,0,0]])
  policy_stable = False
  
  while not policy_stable:
    # Evaluo V
    V = policy_eval_fn(policy, env, theta, discount_factor)

    policy_stable = True

    # Itero por los estados
    for s in states:
      old_action = policy[s].copy()
      action_values = []

      for a in actions:
        
        action_values_aux = []
        
        # Obtengo el valor del juego
        for s_ in [i for i in env.P[s][a]]:          
          action_values_aux.append((s_[0] * (s_[2] + discount_factor*V[s_[1]])))
        
        a_reward = np.sum(action_values_aux)

        action_values.append(a_reward)

      new_s_policy = np.array([0,0,0])
      new_s_policy[np.argmax(action_values)] = 1 
      policy[s] = new_s_policy
      
      if not np.array_equal(old_action, policy[s]):
        policy_stable = False
  return (V, policy)

policy_iteration(env, policy_evaluation, discount_factor)

"""# Utilizando los 3 algoritmos, realice los experimentos para las siguientes configuraciones del ambiente."""

exp1 = generar_ambiente(alpha=0.9, beta=0.9, r_search=3, r_wait=2)
exp2 = generar_ambiente(alpha=0.8, beta=0.5, r_search=3, r_wait=2)
exp3 = generar_ambiente(alpha=0.5, beta=0.5, r_search=3, r_wait=2)
exp4 = generar_ambiente(alpha=0.9, beta=0.6, r_search=1, r_wait=0.9)
exp5 = generar_ambiente(alpha=0.9, beta=0.6, r_search=1, r_wait=0.5)

envs_list = [exp1, exp2, exp3, exp4, exp5]

exp = 1
vi_list = []
pi_list = []

for e in envs_list:
  print("Para el experimento:", exp, " se tienen los siguientes resultados:")
  vi = value_iteration(e, theta, discount_factor)
  vi_list.append(vi)
  pi = policy_iteration(e, policy_evaluation, discount_factor)
  pi_list.append(pi)
  
  print("La política óptima es la misma policy iteration y value iteration?", np.array_equal(vi[0], pi[0]))
  print("El valor de política óptima es la misma policy iteration y value iteration?", np.array_equal(vi[1], pi[1]))
  print("El resultado es:", vi)
  exp += 1

"""# Utilizando el grafico de recompensa, compare las estrategias óptimas generadas con los experimentos anteriores contra la estrategia al azar."""

def simulate(action_type, env, policy=None, n_iter=20, gamma = 0.9):
  observation = env.reset()
  reward_acum = 0
  r_history = []

  # 20 iteraciones
  for i in range(n_iter):

    # Elijo una acción al azar
    if action_type == "sample":
      action = env.action_space.sample()

      # Si estoy en el estado high y tomo recharge, vuelvo a elegir
      while (action == 2 and observation == 0):
        action = env.action_space.sample()
    
    else:
      action = np.argmax(policy[observation])
    # Tomo la acción elegida
    observation, reward, done, info = env.step(action)

    # Actualizo el reward acumulado
      # Actualizo el reward acumulado
    reward_acum = reward_acum + (gamma**i) * reward
    # Guardo el reward en una lista
    r_history.append((i, reward, reward_acum))
  env.close()
  return r_history

e = envs_list[1]

e.nS

random_policy_hist = []
optimal_policy_hist = []
optimal_policy = [i[1] for i in vi_list]
for e in range(len(envs_list)):
  random_policy_hist.append(simulate("sample", envs_list[e], n_iter=40))
  optimal_policy_hist.append(simulate("optimal", envs_list[e], policy=optimal_policy[e],n_iter=40))

len(random_policy_hist[0])

trace1_r = go.Scatter(x = [i[0] for i in random_policy_hist[0]],
                    y = [i[2] for i in random_policy_hist[0]],
                    mode = 'lines', name = 'random policy', showlegend = False, 
                      line = dict(color = ('orange')))
trace1_o = go.Scatter(x = [i[0] for i in optimal_policy_hist[0]],
                    y = [i[2] for i in optimal_policy_hist[0]],
                    mode = 'lines', name = 'optimal policy', showlegend = False,
                      line = dict(color = ('green')))
trace2_r = go.Scatter(x = [i[0] for i in random_policy_hist[1]],
                    y = [i[2] for i in random_policy_hist[1]],
                    mode = 'lines', name = 'random policy', showlegend = False,
                      line = dict(color = ('orange')))
trace2_o = go.Scatter(x = [i[0] for i in optimal_policy_hist[1]],
                    y = [i[2] for i in optimal_policy_hist[1]],
                    mode = 'lines', name = 'optimal policy', showlegend = False,
                      line = dict(color = ('green')))
trace3_r = go.Scatter(x = [i[0] for i in random_policy_hist[2]],
                    y = [i[2] for i in random_policy_hist[2]],
                    mode = 'lines', name = 'random policy', showlegend = False,
                      line = dict(color = ('orange')))
trace3_o = go.Scatter(x = [i[0] for i in optimal_policy_hist[2]],
                    y = [i[2] for i in optimal_policy_hist[2]],
                    mode = 'lines', name = 'optimal policy', showlegend = False,
                      line = dict(color = ('green')))
trace4_r = go.Scatter(x = [i[0] for i in random_policy_hist[3]],
                    y = [i[2] for i in random_policy_hist[3]],
                    mode = 'lines', name = 'random policy', showlegend = False,
                      line = dict(color = ('orange')))
trace4_o = go.Scatter(x = [i[0] for i in optimal_policy_hist[3]],
                    y = [i[2] for i in optimal_policy_hist[3]],
                    mode = 'lines', name = 'optimal policy', showlegend = False,
                      line = dict(color = ('green')))
trace5_r = go.Scatter(x = [i[0] for i in random_policy_hist[4]],
                    y = [i[2] for i in random_policy_hist[4]],
                    mode = 'lines', name = 'random policy', line = dict(color = ('orange')))
trace5_o = go.Scatter(x = [i[0] for i in optimal_policy_hist[4]],
                    y = [i[2] for i in optimal_policy_hist[4]],
                    mode = 'lines', name = 'optimal policy', line = dict(color = ('green')))

from plotly import tools

fig = tools.make_subplots(rows=5, cols=1, subplot_titles=('Experimento 1', 'Experimento 2',
                                                          'Experimento 3', 'Experimento 4',
                                                         'Experimento 5'))

fig.append_trace(trace1_r,1,1)
fig.append_trace(trace1_o,1,1)
fig.append_trace(trace2_r,2,1)
fig.append_trace(trace2_o,2,1)
fig.append_trace(trace3_r,3,1)
fig.append_trace(trace3_o,3,1)
fig.append_trace(trace4_r,4,1)
fig.append_trace(trace4_o,4,1)
fig.append_trace(trace5_r,5,1)
fig.append_trace(trace5_o,5,1)

enable_plotly_in_cell() # Nota: solamente es necesario si se ejecuta en Collaboratory

fig['layout'].update(height=1000, width=1000, title='Experimentos')

iplot(fig)

